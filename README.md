# AI-Analyst v2.2: Платформа для Интеллектуального Анализа Данных и Обнаружения Аномалий

**AI-Analyst - это платформа на базе микросервисов, предназначенная для комплексного анализа табличных данных (CSV) с упором на выявление аномалий и потенциально мошеннических операций. Она объединяет автоматическое профилирование, AI-анализ с использованием LLM (Groq), обучение моделей машинного обучения (с Feature Engineering) и инструменты ручной проверки.**

## Ключевые Возможности

* Загрузка данных: Поддержка загрузки больших CSV-файлов через веб-интерфейс.
* Автоматическое Профилирование: Генерация детальных HTML-отчетов о качестве и структуре данных с помощью ydata-profiling.
* AI-Анализ (Groq): Получение автоматических инсайтов, выявление аномалий, идей для Feature Engineering и оценка рисков с использованием LLM Groq (llama-3.1-8b-instant).
* Интерактивный Чат с AI: Возможность задавать уточняющие вопросы AI-аналитику на основе первоначального отчета.
* Обучение Моделей Обнаружения Аномалий: Обучение моделей без учителя (IsolationForest, LocalOutlierFactor, OneClassSVM) прямо из интерфейса.
* Автоматический Feature Engineering: Опциональная генерация признаков (время суток, агрегаты по карте за час, время с последней транзакции, отклонение от среднего) для улучшения качества моделей.
* Предсказание и Оценка: Применение обученных моделей для оценки аномальности как всего файла (с визуализацией распределения оценок), так и отдельных записей.
* Ручная Проверка Данных: Быстрая проверка телефонов, email, URL и текстов на признаки мошенничества с использованием NLP (spaCy) и балльной системы оценки риска.
* Черные Списки (PostgreSQL): Хранение черных списков в базе данных PostgreSQL.
* Обратная Связь: Возможность пополнять черные списки через интерфейс ручной проверки.
* Информативный Дашборд: Главная страница отображает список загруженных файлов и обученных моделей.
* Интерактивные Визуализации: Использование Plotly для отображения распределений и аномалий.

## Архитектура

* Платформа использует микросервисную архитектуру, оркестрируемую с помощью Docker Compose.
* frontend: Пользовательский интерфейс на Streamlit.
* file_service: Обработка загрузки файлов (FastAPI).
* profiling_service: Генерация отчетов ydata-profiling (FastAPI).
* groq_service: Взаимодействие с Groq API (FastAPI).
* training_service: Обучение ML-моделей (FastAPI, Scikit-learn).
* prediction_service: Применение ML-моделей (FastAPI, Scikit-learn).
* fraud_check_service: Ручная проверка данных (FastAPI, spaCy, SQLAlchemy).
* db: База данных PostgreSQL для черных списков.
* Volumes: Docker Volumes (uploads_data, models_data, reports_data, postgres_data) для обмена данными между сервисами.

## Технологический Стек

* Язык: Python 3.10+
* Backend: FastAPI, Uvicorn
* Frontend: Streamlit
* База данных: PostgreSQL 14+, SQLAlchemy, psycopg2-binary
* ML/Анализ: Scikit-learn, Pandas, Joblib, ydata-profiling
* NLP: spaCy (ru_core_news_sm)
* AI (LLM): Groq API
* Контейнеризация: Docker, Docker Compose
* Другое: Requests, Pydantic, phonenumbers, validators

## Структура Проекта (Основные компоненты)

/ai_analyst_v2/

├── docker-compose.yml     # Файл оркестровки Docker

├── .env                  # Файл с переменными окружения (API-ключи, настройки БД) - СОЗДАТЬ!

│

├── frontend/              # Сервис Frontend (Streamlit)

│   ├── Dockerfile

│   ├── requirements.txt

│   └── src/

│       ├── app.py         # Главная страница (Дашборд)

│       └── pages/         # Остальные страницы приложения

│           ├── 0_Data_Profile.py

│           ├── 1_AI_Analyst_Report.py

│           ├── 2_Train_Model.py

│           ├── 3_Prediction.py

│           └── 4_Fraud_Check.py

│

├── file_service/          # Сервис загрузки файлов

│   ├── Dockerfile

│   ├── requirements.txt

│   └── main.py

│

├── profiling_service/     # Сервис профилирования

│   ├── ... (Dockerfile, reqs, main.py) ...

│

├── groq_service/          # Сервис AI-анализа

│   ├── ... (Dockerfile, reqs, main.py) ...

│

├── training_service/      # Сервис обучения моделей

│   ├── ... (Dockerfile, reqs, main.py) ...

│

├── prediction_service/    # Сервис предсказания

│   ├── ... (Dockerfile, reqs, main.py) ...

│

└── fraud_check_service/   # Сервис ручной проверки

        ├── Dockerfile

        ├── requirements.txt

        ├── main.py

        ├── wait-for-it.sh     # Скрипт ожидания БД




##  Установка и Запуск

#### Предварительные требования:

Установленный Docker
Установленный Docker Compose

##### Клонировать репозиторий:

git clone <URL_вашего_репозитория>
cd ai_analyst_v2

##### Создать файл .env: 

В корневой папке проекта (ai_analyst_v2/) создайте файл с именем .env и добавьте в него следующие переменные, заменив значения на ваши:

### Ключ для Groq API (получите на groq.com)
GROQ_API_KEY=gsk_ВАШ_КЛЮЧ

### Настройки для PostgreSQL (можете оставить по умолчанию)
POSTGRES_USER=user

POSTGRES_PASSWORD=password

POSTGRES_DB=ai_analyst

POSTGRES_HOST=db

Собрать и запустить контейнеры:
Выполните в терминале (в корневой папке проекта):

docker-compose up --build -d


--build: Пересобирает образы, если код или Dockerfile изменились.

-d: Запускает контейнеры в фоновом режиме.

Первый запуск может занять некоторое время из-за скачивания образов и установки зависимостей (включая spaCy).

Доступ к приложению:

Веб-интерфейс (Frontend) будет доступен по адресу: http://localhost:8501

Документация API бэкенд-сервисов (Swagger UI) доступна по их внешним портам (если они проброшены в docker-compose.yml, например, http://localhost:8005/docs для fraud_check_service).

(Опционально) Наполнить БД тестовыми данными:

Откройте Swagger UI для fraud_check_service: http://localhost:8005/docs

Найдите эндпоинт POST /test-data/, нажмите "Try it out", затем "Execute". Это добавит несколько записей в черные списки.

## ️ Использование

Откройте Дашборд: Перейдите на http://localhost:8501.

Загрузите файл: В секции "Загрузка и анализ нового файла" выберите ваш CSV-файл и нажмите "Начать загрузку и анализ". Дождитесь завершения автоматических анализов.

Изучите Data Profile: Перейдите на страницу "Data Profile", чтобы увидеть детальный отчет о данных.

Изучите AI-Отчет: Перейдите на "AI Analyst Report". Просмотрите выводы AI, графики (Box Plot, Scatter Plot аномалий). Задайте уточняющие вопросы в чате.

Обучите Модель: Перейдите на "Train Model". Выберите файл, настройте признаки (числовые, категориальные, даты), опционально включите Feature Engineering (выбрав нужные колонки), введите имя модели, выберите алгоритм (IsolationForest, LOF, OneClassSVM) и нажмите "Начать обучение".

Используйте Модель: Перейдите на "Prediction".

Вкладка "Анализ всего файла": Выберите файл и обученную модель, нажмите "Рассчитать..." для получения гистограммы распределения оценок аномальности.

Вкладка "Одиночная оценка": Выберите модель, введите данные транзакции вручную и получите оценку аномальности.

Проверьте Данные Вручную: Перейдите на "Fraud Check". Выберите тип данных, введите значение, нажмите "Проверить". Если считаете результат неверным, нажмите "Пометить и добавить в черный список".

##  Конфигурация

Все основные настройки (API-ключи, параметры подключения к БД) находятся в файле .env в корне проекта.

##  Возможные Улучшения

Добавление поддержки других моделей ML (например, Autoencoders).

Реализация более сложных техник Feature Engineering.

Интеграция с системами оповещения/реагирования.

Внедрение MLOps практик (мониторинг моделей, переобучение).

Добавление unit- и интеграционных тестов.

Настройка централизованного логирования и мониторинга (ELK, Prometheus/Grafana).